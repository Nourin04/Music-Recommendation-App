# app.py
import streamlit as st
import pandas as pd
import re
import nltk
import joblib
import logging
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("app.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Preprocessing Section
logging.info("üöÄ Starting preprocessing...")

# Load and sample dataset
try:
    df = pd.read_csv("spotify_millsongdata.csv").sample(10000)
    logging.info("‚úÖ Dataset loaded and sampled: %d rows", len(df))
except Exception as e:
    logging.error("‚ùå Failed to load dataset: %s", str(e))
    raise e

# Drop link column and preprocess
df = df.drop(columns=['link'], errors='ignore').reset_index(drop=True)

# Text cleaning
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r"[^a-zA-Z\s]", "", str(text))
    text = text.lower()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

logging.info("üßπ Cleaning text...")
df['cleaned_text'] = df['text'].apply(preprocess_text)
logging.info("‚úÖ Text cleaned.")

# Vectorization
logging.info("üî† Vectorizing using TF-IDF...")
tfidf = TfidfVectorizer(max_features=5000)
tfidf_matrix = tfidf.fit_transform(df['cleaned_text'])
logging.info("‚úÖ TF-IDF matrix shape: %s", tfidf_matrix.shape)

# Cosine similarity
logging.info("üìê Calculating cosine similarity...")
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
logging.info("‚úÖ Cosine similarity matrix generated.")

# Save processed data for future use (optional)
joblib.dump(df, 'df_cleaned.pkl')
joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')
joblib.dump(cosine_sim, 'cosine_sim.pkl')
logging.info("üíæ Data saved to disk.")

# Recommendation function
def recommend_songs(song_name, top_n=5):
    logging.info("üéµ Recommending songs for: '%s'", song_name)
    idx = df[df['song'].str.lower() == song_name.lower()].index
    if len(idx) == 0:
        logging.warning("‚ö†Ô∏è Song not found in dataset.")
        return None
    idx = idx[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n + 1]
    song_indices = [i[0] for i in sim_scores]
    logging.info("‚úÖ Top %d recommendations ready.", top_n)
    
    # Create DataFrame with clean serial numbers starting from 1
    result_df = df[['artist', 'song']].iloc[song_indices].reset_index(drop=True)
    result_df.index = result_df.index + 1  # Start from 1 instead of 0
    result_df.index.name = "S.No."

    return result_df

# Streamlit App Section
st.set_page_config(
    page_title="Music Recommender üéµ",
    page_icon="üéß",
    layout="centered"
)

st.title("üé∂ Instant Music Recommender")

song_list = sorted(df['song'].dropna().unique())
selected_song = st.selectbox("üéµ Select a song:", song_list)

if st.button("üöÄ Recommend Similar Songs"):
    with st.spinner("Finding similar songs..."):
        recommendations = recommend_songs(selected_song)
        if recommendations is None:
            st.warning("Sorry, song not found.")
        else:
            st.success("Top similar songs:")
            st.table(recommendations)
